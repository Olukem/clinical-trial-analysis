I explored different parts of the Apache Spark by using the RDDs, SQL, and DataFrames to process and analyze data. 
The RDD notebook was all about transforming and performing actions on large datasets, giving me hands-on experience with Sparkâ€™s core distributed computing tools. 
In the SQL notebook, i focused on running queries, aggregating data, and performing joins.
The DataFrame notebook, i cleaned and transformed data using PySpark, and used Bokeh to visualize the results interactively.
